
0 training losses:  0.0003021553857252002 9.746948813926792e-05 9.74694871729298e-05 9.746948762767715e-05 9.746948695266155e-06
/local/scratch/PycharmProjects/loss_grad/NCA_v2/NCA_Solidification/NCA_v2/RegP_classEA_t_encode_diCNN/3EA/3D/JIan_data/adddata_5l256_dp0.5/mgpu_ppre_t_grad.py:534: RuntimeWarning: invalid value encountered in arccos
  theta[:,:,i]=np.arccos(0.5*(t1[:,:,i]+t2[:,:,i]+t3[:,:,i]-1))
/local/home/jiatang/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv3d(
/local/home/jiatang/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 264.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv3d(
/local/home/jiatang/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return F.conv3d(
0 training losses:  0.0021364621352404356 0.0006891813236507005 0.0006891813254696899 0.0006891813291076687 6.891813313814055e-05
0 training losses:  0.001353706233203411 0.0004366793637018418 0.0004366793637018418 0.0004366793582448736 4.3667936324709444e-05
0 training losses:  0.0010012241546064615 0.000322975511153345 0.000322975511153345 0.0003229755166103132 3.229755145639501e-05
0 training losses:  0.0008217418217100203 0.000265078061602253 0.000265078061602253 0.0002650780625117477 2.650780663771002e-05
0 training losses:  0.0008755933376960456 0.00028244947952771327 0.00028244947679922916 0.00028244947407074505 2.8244948452993413e-05
0 training losses:  0.0006125180516391993 0.00019758647749767988 0.00019758647931666928 0.00019758648159040604 1.9758648249990074e-05
0 training losses:  0.0006751378532499075 0.00021778638512159887 0.00021778638677005802 0.00021778638438263442 2.1778638767955272e-05

0 training losses:  0.0010661585256457329 0.00034392211750855495 0.0003439221186454233 0.00034392212842249137 3.43922124841356e-05
0 training losses:  0.001046271063387394 0.00033750676857380313 0.00033750676107047184 0.00033750676084309816 3.3750676891486364e-05
0 training losses:  0.0008198874420486391 0.0002644798092887868 0.0002644798083792921 0.0002644798027517936 2.6447980964405815e-05
0 training losses:  0.0005308612599037588 0.00017124555276382125 0.0001712455564870652 0.0001712455514280009 1.712455561531101e-05
0 training losses:  0.00040470450767315924 0.00013054985805638353 0.00013054985447524814 0.00013054985248572848 1.3054985899429994e-05
0 training losses:  0.0003765488800127059 0.00012146739527452155 0.00012146739936724771 0.00012146739823037933 1.2146739891250036e-05
0 training losses:  0.00041109148878604174 0.0001326101823906356 0.00013261018261800928 0.0001326101787526568 1.3261018494858945e-05
0 training losses:  0.00041281923768110573 0.00013316747822500474 0.00013316747913449944 0.00013316748277247825 1.3316748038505466e-05
0 training losses:  0.000391403678804636 0.00012625926751752559 0.00012625926251530473 0.00012625927013232285 1.2625926871123738e-05
0 training losses:  0.00043941399781033397 0.00014174645593811874 0.00014174645616549242 0.00014174645508546746 1.4174645954767584e-05
0 training losses:  0.0005068221944384277 0.00016349098586943 0.00016349099234957976 0.00016349098746104573 1.634909936854001e-05
0 training losses:  0.0004623052664101124 0.0001491307351670912 0.00014913072993749665 0.0001491307294827493 1.4913073286493272e-05
0 training losses:  0.000377168704289943 0.00012166732929586033 0.00012166732847163075 0.00012166733290541742 1.2166733462493085e-05

0 training losses:  0.00032060249941423535 0.00010342014400066546 0.0001034201389984446 0.00010342014155639845 1.034201426364234e-05
0 training losses:  0.0003261396777816117 0.00010520635578359361 0.00010520635862576455 0.00010520635669308831 1.0520635896682506e-05
0 training losses:  0.0003479847509879619 0.0001122531597275156 0.00011225315984120243 0.0001122531582495867 1.1225316228546944e-05
0 training losses:  0.00033203247585333884 0.00010710725644003105 0.00010710725098306284 0.00010710725211993122 1.071072568947784e-05
0 training losses:  0.00029523693956434727 9.523772604325131e-05 9.523772195052516e-05 9.523772587272106e-05 9.523772611430559e-06
0 training losses:  0.000366983498679474 0.0001183817756782446 0.00011838177482559331 0.00011838177488243673 1.1838177641720904e-05
0 training losses:  0.0003685728588607162 0.00011889445789847741 0.0001188944626733246 0.0001188944614796128 1.1889446206225784e-05
0 training losses:  0.0003178918268531561 0.00010254575050794301 0.00010254575425960866 0.0001025457507921601 1.0254575407486755e-05
0 training losses:  0.00030637666350230575 9.883117672870867e-05 9.883117706976918e-05 9.883117820663756e-05 9.883117911613226e-06
0 training losses:  0.00032006364199332893 0.0001032463192132127 0.00010324631807634432 0.00010324631614366808 1.0324632128799749e-05
0 training losses:  0.00027738831704482436 8.948011384291021e-05 8.948011225129449e-05 8.948011537768252e-05 8.948011618770124e-06
0 training losses:  0.00030948169296607375 9.983279022662828e-05 9.983278965819409e-05 9.983279238667819e-05 9.983279394276678e-06
0 training losses:  0.0003051194071304053 9.842561064488109e-05 9.842561078698964e-05 9.842561055961596e-05 9.842561237149994e-06

0 training losses:  0.0003224094398319721 0.00010400303324331617 0.00010400302576840659 0.0001040030321348695 1.0400303299462621e-05
0 training losses:  0.00031391761149279773 0.00010126373541652356 0.00010126373359753416 0.00010126373410912493 1.0126373695129587e-05
0 training losses:  0.0003044961194973439 9.822455109542716e-05 9.822454762797861e-05 9.822454586583262e-05 9.822454813956938e-06
0 training losses:  0.0003134759026579559 0.00010112124061834038 0.0001011212455068744 0.00010112124255101662 1.0112124307681825e-05
0 valid losses:  0.0002631418892917736 8.488448112586866e-05 8.488448074928101e-05 8.48844789018699e-05 8.488448109744695e-06
0 training loss:  0.0003134759026579559 valid loss:  0.0002631418892917736 training acc:  51.768520422149116 %   valid accuracy:  60.40959012681159 %
50 training losses:  0.00028941285563632846 9.33589828946424e-05 9.335898153040034e-05 9.335898567996992e-05 9.335898567996992e-06
50 training losses:  0.0002930204209405929 9.452272178123167e-05 9.452272087173696e-05 9.452272138332773e-05 9.452272252019611e-06
50 training losses:  0.00029021810041740537 9.361873458146874e-05 9.361873679836208e-05 9.361873844682123e-05 9.361873843261037e-06
50 training losses:  0.0002824333496391773 9.110753217100864e-05 9.110753177310471e-05 9.110752938568112e-05 9.110753225627377e-06
50 training losses:  0.00027772827888838947 8.958976798112417e-05 8.958976991380041e-05 8.958977150541614e-05 8.958977296913417e-06
50 training losses:  0.00028582883533090353 9.220284306366011e-05 9.220284323419037e-05 9.220284385946798e-05 9.220284606925588e-06
50 training losses:  0.00028383854078128934 9.156083154948647e-05 9.156083291372852e-05 9.156083351058442e-05 9.156083539352267e-06
50 training losses:  0.00026677941787056625 8.605789156490573e-05 8.605788821114402e-05 8.60578893480124e-05 8.605789048488077e-06
50 training losses:  0.0002711827983148396 8.7478316004308e-05 8.747831736855005e-05 8.747831697064612e-05 8.747831834909903e-06
50 training losses:  0.00029242277378216386 9.432991532776214e-05 9.432991788571599e-05 9.432991600988316e-05 9.432991909363864e-06
50 training losses:  0.00028295506490394473 9.127583874146694e-05 9.12758395372748e-05 9.127583962253993e-05 9.127584043255865e-06
50 training losses:  0.0002775297034531832 8.952570124165504e-05 8.952569908160513e-05 8.952569965003931e-05 8.952570155429385e-06
50 training losses:  0.000271362456260249 8.753625962754086e-05 8.753625985491453e-05 8.753625746749094e-05 8.75362597696494e-06
50 training losses:  0.0002709112595766783 8.739072620755906e-05 8.739072751495769e-05 8.73907269465235e-05 8.739072903551914e-06
50 training losses:  0.0002954625815618783 9.531050230293658e-05 9.531050179134581e-05 9.531050329769641e-05 9.53105043066671e-06
50 training losses:  0.0003078061854466796 9.929231529781646e-05 9.929231524097304e-05 9.929231620731116e-05 9.929231758576407e-06
50 training losses:  0.0002744286903180182 8.852538729797743e-05 8.852538792325504e-05 8.852538940118393e-05 8.852538904591256e-06
50 training losses:  0.00026961188996210694 8.697159046278102e-05 8.697158727954957e-05 8.697158773429692e-05 8.697159167070367e-06
50 training losses:  0.0002763201482594013 8.91355320504772e-05 8.913553563161258e-05 8.913553216416403e-05 8.913553394052087e-06
50 training losses:  0.00029279905720613897 9.445130456242623e-05 9.445130353924469e-05 9.445130609719854e-05 9.445130501717358e-06
50 training losses:  0.0002752605942077935 8.879375343440188e-05 8.879375553760838e-05 8.879375388914923e-05 8.879375378967325e-06
50 training losses:  0.0002771120343822986 8.939098893279152e-05 8.939098802329681e-05 8.939098654536792e-05 8.939098982807536e-06
50 training losses:  0.00029165158048272133 9.408116346776296e-05 9.408116159193014e-05 9.408116284248536e-05 9.408116504516784e-06
50 training losses:  0.00029234043904580176 9.430337522076115e-05 9.430337900084851e-05 9.43033738565191e-05 9.43033786882097e-06
50 training losses:  0.0002663023769855499 8.590398721253223e-05 8.590398817887035e-05 8.59039889178348e-05 8.590398905994334e-06
50 training losses:  0.000260876928223297 8.415383132387433e-05 8.415383126703091e-05 8.415382978910202e-05 8.415383163651313e-06
50 training losses:  0.0002675954019650817 8.632108222172974e-05 8.63210802890535e-05 8.632108466599675e-05 8.632108489337043e-06
50 training losses:  0.00028192985337227583 9.094510531326705e-05 9.094510571117098e-05 9.094510804175115e-05 9.09451071962053e-06
50 training losses:  0.00027364963898435235 8.827408180422935e-05 8.827408015577021e-05 8.827408481693055e-05 8.82740835805862e-06
50 training losses:  0.0002736576134338975 8.827666215438512e-05 8.827665919852734e-05 8.827665470789725e-05 8.82766596532747e-06
50 training losses:  0.00028214111807756126 9.101326145355415e-05 9.101326122618048e-05 9.10132609988068e-05 9.101326405414056e-06
50 training losses:  0.0002696906158234924 8.699697366409964e-05 8.699697093561554e-05 8.69969714472063e-05 8.699697183089938e-06
50 training losses:  0.0002808142453432083 9.05852486994263e-05 9.05852505184157e-05 9.058525205318801e-05 9.058525236582682e-06
50 training losses:  0.0002708470856305212 8.737001934377986e-05 8.737001758163387e-05 8.737001786585097e-05 8.737001969905123e-06
50 training losses:  0.0002802441595122218 9.040135194027243e-05 9.040135387294868e-05 9.040135148552508e-05 9.040135314108966e-06

50 training losses:  0.00027829600730910897 8.97728909876605e-05 8.977288666756067e-05 8.977289030553948e-05 8.977289237321884e-06
50 training losses:  0.0002688491658773273 8.672554506006236e-05 8.672554122313159e-05 8.67255412515533e-05 8.67255442926762e-06
50 training losses:  0.00028643407858908176 9.239808929351057e-05 9.239808628080937e-05 9.239808736083432e-05 9.23980889666609e-06
50 valid losses:  0.0002539224767019732 8.191047592731593e-05 8.191047852790234e-05 8.191047756156422e-05 8.191047858652212e-06
50 training loss:  0.00028643407858908176 valid loss:  0.0002539224767019732 training acc:  71.23894942434212 %   valid accuracy:  63.78226902173911 %
100 training losses:  0.00032679695868864655 0.00010541837929167741 0.00010541837571054202 0.00010541837559685518 1.0541837848876412e-05
100 training losses:  0.000320462160743773 0.00010337489780454234 0.00010337489291600832 0.00010337489305811687 1.0337489914746811e-05
100 training losses:  0.000310605566482991 0.00010019532857086233 0.00010019532720662028 0.00010019532993510438 1.0019533039695716e-05
100 training losses:  0.0002811409649439156 9.069065720268554e-05 9.06906546447317e-05 9.069065538369614e-05 9.06906587516687e-06
100 training losses:  0.00027505835168994963 8.872849821273121e-05 8.872850003172061e-05 8.872849559793394e-05 8.872849832641805e-06
100 training losses:  0.00029039010405540466 9.367421273509535e-05 9.367421091610595e-05 9.367421284878219e-05 9.367421441197621e-06

100 training losses:  0.0002800922666210681 9.035234023713201e-05 9.035233927079389e-05 9.035234074872278e-05 9.035234029397543e-06
100 training losses:  0.00033671350684016943 0.00010861726096322855 0.00010861726562438889 0.0001086172638338212 1.0861726494226787e-05
100 training losses:  0.0003887584898620844 0.00012540600320676276 0.00012540599593080515 0.00012540600440047456 1.2540600458521567e-05
100 training losses:  0.0003545868385117501 0.00011438282268727562 0.00011438282493259067 0.0001143828226304322 1.1438282569997682e-05
100 training losses:  0.0002676911244634539 8.635196081741014e-05 8.635196024897596e-05 8.635195933948125e-05 8.635196053319305e-06
100 training losses:  0.00029497823561541736 9.515429246675922e-05 9.515429258044605e-05 9.515428996564879e-05 9.515429326256708e-06
100 training losses:  0.000316343066515401 0.00010204614670783485 0.00010204613943187724 0.00010204614341091656 1.0204614483200203e-05
100 training losses:  0.0002831024175975472 9.132336498396398e-05 9.13233662345192e-05 9.132336640504946e-05 9.132336735717672e-06
100 training losses:  0.0003469673974905163 0.00011192495446721296 0.00011192495364298338 0.00011192495409773073 1.119249554903945e-05
100 training losses:  0.00035660102730616927 0.00011503258474476752 0.00011503258281209128 0.00011503258082257162 1.1503258772194158e-05
100 training losses:  0.0002848847070708871 9.189830689138034e-05 9.189830413447453e-05 9.189830748823624e-05 9.189830837641466e-06
100 training losses:  0.000291919510345906 9.416756597602216e-05 9.416756449809327e-05 9.41675642707196e-05 9.416756540758797e-06
100 training losses:  0.00031781563302502036 0.00010252117078834999 0.00010252116737774486 0.00010252116953779478 1.0252117064624144e-05
100 training losses:  0.0002809990255627781 9.064486857823795e-05 9.064486778243008e-05 9.064486727083931e-05 9.064486789611692e-06
100 training losses:  0.00032545984140597284 0.00010498704071437714 0.00010498703684902466 0.00010498703977646073 1.049870419222998e-05
100 training losses:  0.0003565768420230597 0.00011502478082547896 0.00011502477900648955 0.00011502478218972101 1.1502478251657067e-05
100 training losses:  0.00029463579994626343 9.504381850433674e-05 9.504381955593999e-05 9.504382046543469e-05 9.504382340708162e-06
100 training losses:  0.0002990488428622484 9.646735247770266e-05 9.646735372825788e-05 9.64673525913895e-05 9.646735549040386e-06
100 training losses:  0.0003574671281967312 0.00011531199106684653 0.00011531199015735183 0.00011531199015735183 1.1531199163528072e-05
100 training losses:  0.0002904395223595202 9.369017391236412e-05 9.36901708428195e-05 9.369017334392993e-05 9.369017618610087e-06
100 training losses:  0.0002938093966804445 9.477723108375358e-05 9.47772266499669e-05 9.477723003215033e-05 9.477723107664815e-06
100 training losses:  0.00034907806548289955 0.00011260582888894533 0.00011260583164585114 0.00011260583301009319 1.1260583406169644e-05
100 training losses:  0.00032298118458129466 0.00010418748306051384 0.00010418747996254751 0.00010418748431106906 1.041874844531776e-05
100 training losses:  0.00030056637478992343 9.695688049760065e-05 9.69568830555545e-05 9.69568830555545e-05 9.695688291344595e-06
100 training losses:  0.0003047137288376689 9.829475777678454e-05 9.829475823153189e-05 9.829475629885565e-05 9.829475899891804e-06
100 training losses:  0.00026900676311925054 8.677638186327385e-05 8.677638152221334e-05 8.677638038534496e-05 8.677638312803992e-06
100 training losses:  0.00028681158437393606 9.251986989511352e-05 9.251986753611163e-05 9.251986938352275e-05 9.251987091118963e-06
100 training losses:  0.0003041069721803069 9.809902257984504e-05 9.809902405777393e-05 9.809902385882197e-05 9.809902611124244e-06
100 training losses:  0.00028859981102868915 9.309670437573914e-05 9.309670411994375e-05 9.309670747370546e-05 9.309670598867115e-06
100 training losses:  0.000287924543954432 9.287888462949923e-05 9.287888491371632e-05 9.28788878695741e-05 9.287888779851983e-06
100 training losses:  0.0002609899966046214 8.419033571271939e-05 8.419033321160896e-05 8.419033429163392e-05 8.419033441953161e-06
100 training losses:  0.0002733785950113088 8.818663772558466e-05 8.818663823717543e-05 8.818664167620227e-05 8.818663957299577e-06
100 valid losses:  0.00027293958450513855 8.804502705572759e-05 8.804502726889041e-05 8.804502608228404e-05 8.804502810377812e-06
100 training loss:  0.0002733785950113088 valid loss:  0.00027293958450513855 training acc:  74.87578810307018 %   valid accuracy:  64.9498980978261 %

150 training losses:  0.0002747843973338604 8.86401224988731e-05 8.864012346521122e-05 8.864012437470592e-05 8.864012414733224e-06
150 training losses:  0.0002595259284134954 8.371804364060154e-05 8.371804750595402e-05 8.371804722173692e-05 8.371804739226718e-06
150 training losses:  0.00027106585912406445 8.744060619392258e-05 8.744060460230685e-05 8.744060539811471e-05 8.744060693288702e-06
150 training losses:  0.0002608446520753205 8.414344711127342e-05 8.414344273433016e-05 8.4143442848017e-05 8.41434463794144e-06
150 training losses:  0.0002738159673754126 8.83277376146907e-05 8.832773741573874e-05 8.83277384957637e-05 8.83277396113158e-06
150 training losses:  0.0002896451042033732 9.343390215121872e-05 9.343390590288436e-05 9.343390357230419e-05 9.343390519944705e-06
150 training losses:  0.0002596601552795619 8.376133177989686e-05 8.376133240517447e-05 8.376133132514951e-05 8.37613328030784e-06
150 training losses:  0.00026171852368861437 8.442532092090005e-05 8.442532254093749e-05 8.442532308094997e-05 8.442532291041971e-06
150 training losses:  0.0002822537499014288 9.104960989247957e-05 9.104960616923563e-05 9.104961011985324e-05 9.104961037564863e-06
150 training losses:  0.0002786463301163167 8.988591486058795e-05 8.988591571323923e-05 8.988591304159854e-05 8.988591616798658e-06
150 training losses:  0.00028872417169623077 9.31368247734099e-05 9.313682824085845e-05 9.313682414813229e-05 9.313682738820717e-06
150 training losses:  0.0002830062003340572 9.129232478244376e-05 9.12923237024188e-05 9.129232336135829e-05 9.129232488191974e-06
150 training losses:  0.0002821104717440903 9.100338385792384e-05 9.100338289158572e-05 9.100338365897187e-05 9.100338516532247e-06
150 training losses:  0.0002682069898582995 8.651836947137781e-05 8.65183709493067e-05 8.651837194406653e-05 8.65183710274664e-06
150 training losses:  0.00026460731169208884 8.535720860436413e-05 8.535720775171285e-05 8.535720616009712e-05 8.53572074532849e-06
150 training losses:  0.00026099232491105795 8.419107092549893e-05 8.419107149393312e-05 8.419107086865552e-05 8.419107103918577e-06
150 training losses:  0.0002634111442603171 8.497132381535266e-05 8.497132239426719e-05 8.49713212005554e-05 8.4971323488503e-06

150 training losses:  0.0002834017213899642 9.141989943373119e-05 9.141989926320093e-05 9.141990079797324e-05 9.141989984584598e-06
150 training losses:  0.00031334138475358486 0.00010107786607704838 0.0001010778607053453 0.00010107786468438462 1.010778661836298e-05
150 training losses:  0.00029766871011815965 9.602215109794088e-05 9.602215706649986e-05 9.602215558857097e-05 9.602215612858345e-06
150 training losses:  0.00026172216166742146 8.442650266715646e-05 8.442650050710654e-05 8.442650107554073e-05 8.442650297979526e-06
150 training losses:  0.00026925362180918455 8.685600755597989e-05 8.685600721491937e-05 8.685600641911151e-05 8.68560078970404e-06
150 training losses:  0.00028518453473225236 9.199500880185951e-05 9.199500675549643e-05 9.199500743761746e-05 9.199500851764242e-06
150 training losses:  0.00028129201382398605 9.073933776448939e-05 9.073934083403401e-05 9.073933887293606e-05 9.07393411964108e-06
150 training losses:  0.0003075312706641853 9.920364351501121e-05 9.920364746562882e-05 9.920364627191702e-05 9.920364901105927e-06
150 training losses:  0.0002940421982202679 9.485234258477249e-05 9.485234178896462e-05 9.485234417638821e-05 9.485234432560219e-06
150 training losses:  0.0002542575530242175 8.201854734579683e-05 8.20185508700888e-05 8.201854922162966e-05 8.201855038691974e-06
150 training losses:  0.00027031905483454466 8.719969446246978e-05 8.719969343928824e-05 8.719969628145918e-05 8.71996954288079e-06
150 training losses:  0.00026472299941815436 8.539452005607018e-05 8.539451994238334e-05 8.539451766864659e-05 8.53945225287589e-06
150 training losses:  0.0003059037262573838 9.867862041801345e-05 9.86786199632661e-05 9.867862161172525e-05 9.86786228196479e-06
150 training losses:  0.00032067246502265334 0.00010344273570694895 0.0001034427334616339 0.00010344273414375493 1.0344273658091652e-05
150 training losses:  0.00026507244911044836 8.550725027589579e-05 8.550724794531561e-05 8.550724737688142e-05 8.55072494232445e-06
150 training losses:  0.00027878739638254046 8.993140852453507e-05 8.993140841084823e-05 8.993140716029302e-05 8.993140994562054e-06
150 training losses:  0.0002869246818590909 9.255634404325974e-05 9.255634336113872e-05 9.255634472538077e-05 9.255634651594846e-06
150 training losses:  0.00026043536490760744 8.40114156517302e-05 8.401141644753807e-05 8.401141377589738e-05 8.401141663227918e-06
150 training losses:  0.0003091279068030417 9.971868064440059e-05 9.971868175284726e-05 9.971868581715171e-05 9.971868362157466e-06
150 training losses:  0.00030805665301159024 9.937312367469531e-05 9.937311816088368e-05 9.937312211150129e-05 9.937312327679138e-06
150 training losses:  0.00027575495187193155 8.895320095803072e-05 8.895320559076936e-05 8.89532032886109e-05 8.89532045889041e-06
150 valid losses:  0.00023133089666771411 7.462287084081254e-05 7.462287109660792e-05 7.46228732708687e-05 7.4622873551533075e-06
150 training loss:  0.00027575495187193155 valid loss:  0.00023133089666771411 training acc:  77.43198327850877 %   valid accuracy:  64.72911005434781 %
EarlyStopping counter: 1 out of 70
200 training losses:  0.00026120265829376876 8.42589089984358e-05 8.425890672469905e-05 8.425890774788058e-05 8.425890928975832e-06
200 training losses:  0.0002582251909188926 8.32984343901444e-05 8.329843581122987e-05 8.329843456067465e-05 8.329843581122987e-06
200 training losses:  0.00026832695584744215 8.655707301841176e-05 8.655707222260389e-05 8.655707114257893e-05 8.655707247839928e-06
200 training losses:  0.00026834409800358117 8.656261684336641e-05 8.656261536543752e-05 8.65626183212953e-05 8.656261776707197e-06
200 training losses:  0.00025797856505960226 8.321889987428222e-05 8.321889868057042e-05 8.321889816897965e-05 8.32189000021799e-06
200 training losses:  0.0002554899256210774 8.24160965748888e-05 8.241609685910589e-05 8.241609725700982e-05 8.241609716463927e-06
200 training losses:  0.0002526550379116088 8.150163392883769e-05 8.15016351793929e-05 8.150163705522573e-05 8.150163708364744e-06
200 training losses:  0.0002570502692833543 8.291944476468416e-05 8.29194457878657e-05 8.291944601523937e-05 8.291944759264425e-06
200 training losses:  0.0002642764011397958 8.52504442150348e-05 8.525044529505976e-05 8.525044273710591e-05 8.525044552953887e-06
200 training losses:  0.000283093104371801 9.132035216907752e-05 9.13203505774618e-05 9.132035324910248e-05 9.132035174275188e-06
200 training losses:  0.00026064497069455683 8.407902748786e-05 8.407903135321249e-05 8.407902618046137e-05 8.407902853946325e-06
200 training losses:  0.0002599182480480522 8.384457368038056e-05 8.384457231613851e-05 8.384457464671868e-05 8.384457537147227e-06
200 training losses:  0.000275688711553812 8.893186523550867e-05 8.893186446812251e-05 8.893186574709944e-05 8.893186731029346e-06
200 training losses:  0.0002748656552284956 8.86663305550428e-05 8.866632853710144e-05 8.866633123716383e-05 8.866633105952815e-06
200 training losses:  0.0002621770545374602 8.457323070842904e-05 8.45732296852475e-05 8.457322911681331e-05 8.457323232846647e-06
200 training losses:  0.0002648079243954271 8.542190511207082e-05 8.54219038615156e-05 8.542190192883936e-05 8.54219062773609e-06
200 training losses:  0.00026091886684298515 8.416737472316527e-05 8.416737011884834e-05 8.416737273364561e-05 8.416737490080095e-06
200 training losses:  0.0002720154880080372 8.774692415158825e-05 8.774692420843166e-05 8.774692273050277e-05 8.774692407342854e-06
200 training losses:  0.00025549015845172107 8.241616956183861e-05 8.241616706072818e-05 8.241616882287417e-05 8.241617067028528e-06
200 training losses:  0.00025927991373464465 8.363869653749134e-05 8.363869244476518e-05 8.363869477534536e-05 8.36386958269486e-06
200 training losses:  0.0002638678124640137 8.511865033256072e-05 8.511865112836858e-05 8.511865041782585e-05 8.51186515760105e-06
200 training losses:  0.00026094081113114953 8.417444203701052e-05 8.417444263386642e-05 8.41744428612401e-05 8.417444327335488e-06
200 training losses:  0.00025538488989695907 8.238222824274999e-05 8.238222909540127e-05 8.238222619638691e-05 8.238222875434076e-06
200 training losses:  0.0002514567459002137 8.111506167551852e-05 8.111506105024091e-05 8.111506122077117e-05 8.11150626844892e-06
200 training losses:  0.00025663524866104126 8.278556907725942e-05 8.27855723173343e-05 8.278557066887515e-05 8.278557267260567e-06
200 training losses:  0.0002517268294468522 8.12022043419347e-05 8.120220275031897e-05 8.120220661567146e-05 8.120220655882804e-06
200 training losses:  0.00025472152628935874 8.216823169959753e-05 8.216823081852453e-05 8.216823212592317e-05 8.216823204065804e-06
200 training losses:  0.0002640365855768323 8.517308697264525e-05 8.517308822320047e-05 8.517309061062406e-05 8.517308955902081e-06
200 training losses:  0.0002540361601859331 8.194714422415927e-05 8.194714433784611e-05 8.194714274623038e-05 8.194714489206945e-06
200 training losses:  0.00027301275986246765 8.806864258303904e-05 8.80686415598575e-05 8.806864383359425e-05 8.806864450150442e-06
200 training losses:  0.00027821838739328086 8.97478715842226e-05 8.974786840099114e-05 8.974787075999302e-05 8.974787029103481e-06
200 training losses:  0.0002755693276412785 8.889332553962959e-05 8.8893325596473e-05 8.889332843864395e-05 8.889332828942997e-06
200 training losses:  0.00025189542793668807 8.125657814161968e-05 8.125657785740259e-05 8.125658047219986e-05 8.125658048641071e-06
200 training losses:  0.0002559144631959498 8.255306840965204e-05 8.255306744331392e-05 8.255306721594025e-05 8.255306823912179e-06
200 training losses:  0.00027596845757216215 8.902207656547034e-05 8.902207628125325e-05 8.902207656547034e-05 8.902207902394821e-06
200 training losses:  0.0002886897709686309 9.312574186992606e-05 9.312574204045632e-05 9.312574306363786e-05 9.31257432057464e-06
200 training losses:  0.000296262587653473 9.556856630865695e-05 9.556856599601815e-05 9.556857062875679e-05 9.556856781500755e-06
200 training losses:  0.0002760478528216481 8.904769538276014e-05 8.904769543960356e-05 8.90476952690733e-05 8.904769877915442e-06
200 valid losses:  0.0002430039565979314 7.838837291274103e-05 7.838837333906667e-05 7.838837166218582e-05 7.838837261076037e-06
200 training loss:  0.0002760478528216481 valid loss:  0.0002430039565979314 training acc:  77.15700383771929 %   valid accuracy:  65.24003623188405 %
250 training losses:  0.00030063363374210894 9.697859798052377e-05 9.697859849211454e-05 9.697860025426053e-05 9.697859969293177e-06
250 training losses:  0.0002847900032065809 9.186774906311257e-05 9.186775142211445e-05 9.186774803993103e-05 9.186775017155924e-06
250 training losses:  0.0002682841441128403 8.654327081103474e-05 8.654327046997423e-05 8.654327314161492e-05 8.654327281476526e-06
250 training losses:  0.0002725539670791477 8.792062010343216e-05 8.792062214979524e-05 8.792062146767421e-05 8.792062132556566e-06
250 training losses:  0.00026744589558802545 8.627287468243594e-05 8.627287479612278e-05 8.627287331819389e-05 8.627287684248586e-06
250 training losses:  0.0002581185253802687 8.326404554281908e-05 8.326404218905736e-05 8.32640444059507e-05 8.326404497438489e-06
250 training losses:  0.0002834727638401091 9.144281040107671e-05 9.144281224848783e-05 9.144281469275484e-05 9.144281253981035e-06
250 training losses:  0.0002666211221367121 8.600681130133125e-05 8.600681542247912e-05 8.600681067605365e-05 8.600681521642173e-06
250 training losses:  0.00027238522307015955 8.786620014689106e-05 8.786619594047806e-05 8.786619707734644e-05 8.786619837053422e-06
250 training losses:  0.00029136775992810726 9.398959946338437e-05 9.398959900863701e-05 9.398959991813172e-05 9.398960138184975e-06
250 training losses:  0.000258126383414492 8.326656973167701e-05 8.326656774215735e-05 8.326656887902573e-05 8.326656818269385e-06
250 training losses:  0.0002846457064151764 9.182119183037685e-05 9.182119387673993e-05 9.18211922851242e-05 9.182119466544236e-06

250 training losses:  0.00027667018002830446 8.924845985802676e-05 8.924846034119582e-05 8.92484593748577e-05 8.924845985802676e-06
250 training losses:  0.0002530325436964631 8.16233903151442e-05 8.162338934880609e-05 8.162338855299822e-05 8.162339103279237e-06
250 training losses:  0.00026417087065055966 8.521640847902745e-05 8.521640791059326e-05 8.521640944536557e-05 8.521640822323207e-06
250 training losses:  0.0002612606331240386 8.427761042639759e-05 8.427760758422664e-05 8.42776116769528e-05 8.427761159168767e-06
250 training losses:  0.00026652394444681704 8.597546440114456e-05 8.597546718647209e-05 8.597546408850576e-05 8.597546798938538e-06
250 training losses:  0.0002619533333927393 8.450108762758646e-05 8.450108848023774e-05 8.45010877412733e-05 8.45010890415665e-06

250 training losses:  0.0002609684888739139 8.41833686422433e-05 8.41833686422433e-05 8.418336841486962e-05 8.418336967963569e-06
250 training losses:  0.0002692144480533898 8.684336808073567e-05 8.684336739861465e-05 8.684336671649362e-05 8.684336947339943e-06
250 training losses:  0.0002611094678286463 8.42288414446557e-05 8.422884286574117e-05 8.422884411629639e-05 8.42288451607942e-06
250 training losses:  0.00025042166817002 8.078117792820194e-05 8.078117770082827e-05 8.078117701870724e-05 8.07811786174284e-06
250 training losses:  0.00025867001386359334 8.344194094433988e-05 8.344194156961748e-05 8.344194532128313e-05 8.344194355913714e-06
250 training losses:  0.00025635407655499876 8.269488699852445e-05 8.26948874532718e-05 8.269488461110086e-05 8.269488692036475e-06
250 training losses:  0.00025493555585853755 8.22372852837816e-05 8.223728573852895e-05 8.223728607958947e-05 8.22372876996269e-06
250 training losses:  0.00025407751672901213 8.19604757111847e-05 8.196047429009923e-05 8.19604736079782e-05 8.196047616593205e-06
250 training losses:  0.00025631749304011464 8.268306231684619e-05 8.268306612535525e-05 8.268306373793166e-05 8.268306473269149e-06
250 training losses:  0.00026354743749834597 8.501530703597382e-05 8.501530629700937e-05 8.501530589910544e-05 8.501530793125767e-06
250 training losses:  0.0002551112847868353 8.229396900105712e-05 8.229396456727045e-05 8.229396661363353e-05 8.229396833314695e-06
250 training losses:  0.0002606798952911049 8.409027685729598e-05 8.409027805100777e-05 8.409027714151307e-05 8.409027856259854e-06

250 training losses:  0.00026572507340461016 8.571775885002353e-05 8.5717756064696e-05 8.571775623522626e-05 8.571775850896302e-06
250 training losses:  0.00026026126579381526 8.395523974513708e-05 8.395524008619759e-05 8.395523948934169e-05 8.395524130122567e-06
250 training losses:  0.0002616679121274501 8.440901382300581e-05 8.440900910500204e-05 8.440901083872632e-05 8.440901176243187e-06
250 training losses:  0.00024497683625668287 7.902478330379381e-05 7.902478131427415e-05 7.902478228061227e-05 7.902478451171646e-06
250 training losses:  0.0002566352195572108 8.278554633989188e-05 8.27855491252194e-05 8.278554878415889e-05 8.278554993523812e-06
250 training losses:  0.00026373795117251575 8.507675204327825e-05 8.507675201485654e-05 8.507675232749534e-05 8.507675513413915e-06
250 training losses:  0.0002672890550456941 8.622227298360485e-05 8.622227178989306e-05 8.622227687737904e-05 8.622227586130293e-06
250 training losses:  0.00025584170361980796 8.252958292587209e-05 8.252958008370115e-05 8.252958184584713e-05 8.252958352272799e-06
250 valid losses:  0.00023614237267111093 7.617495884915115e-05 7.617495791123474e-05 7.617496009970637e-05 7.617496036615989e-06
250 training loss:  0.00025584170361980796 valid loss:  0.00023614237267111093 training acc:  75.57565789473684 %   valid accuracy:  62.8807178442029 %
